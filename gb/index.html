<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Paul Schrimpf">
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Notes -  </title>
        <link href="../css/bootstrap.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/atelier-forest-light.min.css">
        <link href="../assets/Documenter.css" rel="stylesheet">
        <link href="../assets/extra.css" rel="stylesheet">

        <script src="../js/jquery-1.10.2.min.js" defer></script>
        <script src="../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href=".."> </a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href="../index.md" class="nav-link">Package Docs</a>
                            </li>
                            <li class="navitem active">
                                <a href="./" class="nav-link">Notes</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../license/" class="dropdown-item">License</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" class="nav-link disabled">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../license/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li class="nav-item">
                                <a href="https://github.com/schrimpf/NeuralNetworkEconomics.jl/edit/master/docs/gb.md" class="nav-link"><i class="fa fa-github"></i> Edit on GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#introduction" class="nav-link">Introduction</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#decision-trees" class="nav-link">Decision Trees</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#gradient-boosting" class="nav-link">Gradient Boosting</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#gradient-descent-in-function-space" class="nav-link">Gradient Descent in Function Space</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#gradient-boosting-algorithm" class="nav-link">Gradient Boosting Algorithm</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#regularization" class="nav-link">Regularization</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#statistical-properties-and-references" class="nav-link">Statistical Properties and References</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#example-code" class="nav-link">Example Code</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#alternate-baselearner" class="nav-link">Alternate baselearner</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#packages" class="nav-link">Packages</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#bibliography-bibliography" class="nav-link">Bibliography [bibliography]</a>
              <ul class="nav flex-column">
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a></p>
<p>This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike
4.0 International
License</a></p>
<p>
<script type="math/tex; mode=display">
\def\indep{\perp\!\!\!\perp}
\def\Er{\mathrm{E}}
\def\R{\mathbb{R}}
\def\En{{\mathbb{E}_n}}
\def\Pr{\mathrm{P}}
\newcommand{\norm}[1]{\left\Vert {#1} \right\Vert}
\newcommand{\abs}[1]{\left\vert {#1} \right\vert}
</script>
</p>
<h1 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h1>
<p>Variants of gradient boosting decision trees often win prediction
contests (e.g. Bojer and Meldgaard (<a href="#ref-bojer2021">2021</a>) and
Makridakis, Spiliotis, and Assimakopoulos (<a href="#ref-m5accuracy">2022</a>) ).
Yang, Chuang, and Kuan (<a href="#ref-yang2020">2020</a>) examine the peformance of
various estimation methods in double machine learning. They compare
gradient boosted trees, random forests, lasso, neural networks, and
support vector machines, and find that gradient boosting performs best.</p>
<h1 id="decision-trees">Decision Trees<a class="headerlink" href="#decision-trees" title="Permanent link">&para;</a></h1>
<p>See
<a href="https://schrimpf.github.io/NeuralNetworkEconomics.jl/ml-methods/#random-forests">this.</a></p>
<h1 id="gradient-boosting">Gradient Boosting<a class="headerlink" href="#gradient-boosting" title="Permanent link">&para;</a></h1>
<p>Gradient boosting was first described in detail by Friedman
(<a href="#ref-friedman2001">2001</a>), and its stochastic variant in Friedman
(<a href="#ref-friedman2002">2002</a>). Gradient boosting deals with the following
regression or classification problem.</p>
<p>
<script type="math/tex; mode=display">
\min_{F \in \mathcal{F}} E_x \left[ E_y\left[ L(y, F(x)) | x \right] \right]
</script>
</p>
<p>where $L$ is some loss function and $\mathcal{F}$ is some family of
functions. For example if</p>
<p>$L$ is sqaured error and $\mathcal{F}$ is polynomials of $x$, then this
problem becomes series regression.</p>
<p>Gradient boosting focuses on the case where $\mathcal{F}$ is an additive
sum of “simple” learners.</p>
<p>
<script type="math/tex; mode=display">
\mathcal{F} = \{ F(x;\beta, a) = \sum_{m=1}^M \beta_m h(x;a_m) \}
</script>
</p>
<p>For example, if <script type="math/tex; mode=display">
h(x; a_m) = \psi(x'a_m)
</script> where $\psi$ is some activation function, then $\mathcal{F}$ would be
a single layer neural network of width $M$. Gradient boosting is most
often used when $h(x;a_m)$ is a shallow decision tree.</p>
<p>Fully minimizing over $\beta$ and $a$ can be difficult. Therefore,
gradient boosting computes an approximate minimum through a greedy,
stagewise approach.</p>
<h2 id="gradient-descent-in-function-space">Gradient Descent in Function Space<a class="headerlink" href="#gradient-descent-in-function-space" title="Permanent link">&para;</a></h2>
<p>To motivate gradient boosting, consider what gradient descent in the
original problem looks like. Let
$Q(F) = E_x \left[ E_y\left[ L(y,F(x)) | x \right] \right]$ denote the
objective function. Then gradient descent would first choose an initial
$F_0(m)$, and then:</p>
<ol>
<li>
<p>Calculate <script type="math/tex; mode=display">
    \frac{\partial Q(F_{m-1}}{\partial F} = E_y\left[\frac{\partial L(y, F_{m-1}(x))}{\partial F(x)} | x \right]
    </script>
</p>
</li>
<li>
<p>Set $F_m = F_{m-1} - \nu \frac{\partial Q(F_{m-1})}{\partial F}$</p>
</li>
<li>
<p>Repeat</p>
</li>
</ol>
<p>Optionally, the step-size, $\nu$ could be adjusted and/or a line search
could be added.</p>
<h2 id="gradient-boosting-algorithm">Gradient Boosting Algorithm<a class="headerlink" href="#gradient-boosting-algorithm" title="Permanent link">&para;</a></h2>
<p>With the constraint that $F(x) = \sum \beta_m h(x; a_m)$, we cannot
always update $F$ in the direction of the gradient. Instead, we choose a
direction that obeys the constraint and is close to the gradient.
Specifically,</p>
<ol>
<li>Choose $a_m$ to approximate the gradient: <script type="math/tex; mode=display">
    a_m = \textrm{arg}\min_a \sum_{i=1}^N \left( \frac{\partial -L(y_i, F_{m-1}(x_i))}{\partial F(x)} - h(x_i; a) \right)^2
    </script>
</li>
<li>Line search: <script type="math/tex; mode=display">
    \rho_m = \textrm{arg}\min_\rho \sum_{i=1}^N L\left( y_i, F_{m-1}(x_i) + \rho h(x_i;a_m) \right)
    </script>
</li>
<li>Update: <script type="math/tex; mode=display">
    F_m(x) = F_{m-1}(x) + \nu \rho_m h(x; a_m)
    </script>
</li>
<li>Repeat</li>
</ol>
<p>Notice that when $L$ is squared error loss, then step 1 becomes fitting
$h(x_i;a)$ to the residuals. This idea of updating a model by fitting
the residuals existed before Friedman (<a href="#ref-friedman2001">2001</a>) , and
is where the name “boosting” originates.</p>
<h2 id="regularization">Regularization<a class="headerlink" href="#regularization" title="Permanent link">&para;</a></h2>
<p>There are at least three parameters that affect model complexity: the
number of iterations, $M$, the step size / shrinkage parameter, $\nu$,
and the complexity of $h(x,a)$.</p>
<p>In the case of trees, the depth of $h(x,a)$ has a clear implication for
type of functions that the model can well approximate. If $h(x,a)$ are
stubs – trees with a single split, then each $h(x,a)$ is a function of
only one component of $x$. Therefore we should expect to be able to
approximate additively separable functions like</p>
<p>
<script type="math/tex; mode=display">
f_1(x_1) + f_2(x_2) + ... + f_k(x_k)
</script>
</p>
<p>well. Trees with two splits can accomodate pairwise interactions, etc.</p>
<h2 id="statistical-properties-and-references">Statistical Properties and References<a class="headerlink" href="#statistical-properties-and-references" title="Permanent link">&para;</a></h2>
<p>Zhang and Yu (<a href="#ref-zhang2005">2005</a>) has consistency and convergence
rate results. Whether those results are compatible with double machine
learning is not yet clear to me.</p>
<p>Zhou and Hooker (<a href="#ref-zhou2018">2018</a>) also appears relevant. It gives
pointwise asymptotic normality. Like Athey et al’s results for forests,
the class of functions considered is too rich to get fast convergence
rates.</p>
<p>Dombry and Duchamps (<a href="#ref-dombry2021">2021</a>) ODE view of boosting</p>
<p>Yang, Chuang, and Kuan (<a href="#ref-yang2020">2020</a>) has simulation evidence
that gradient boosting outperforms other methods in double machine
learning, but not formal proofs.</p>
<p>In high dimensional settings (i.e. $x$ is high dimensional, and
$h(x,a_m)$ is selecting one covariate) there is a relationship between
boosting and lasso, see Luo and Spindler (<a href="#ref-luo2016">2016</a>) and
Kueck et al. (<a href="#ref-kueck2017">2017</a>).</p>
<p>Note: A creative definition of $x$ as indicators that your base tree can
generate might make Luo and Spindler (<a href="#ref-luo2016">2016</a>) results
applicable to boosting trees. The key would be verifying the sparse
eigenvalue condition A.2, and the restriction that the “true” function
has a sparse representation in terms of these indicators.</p>
<p>Yue, Li, and Sun (<a href="#ref-yue2022">2022</a>) and Bakhitov and Singh
(<a href="#ref-bakhitov2022">2022</a>) combine boosting with instrumental
variables.</p>
<h1 id="example-code">Example Code<a class="headerlink" href="#example-code" title="Permanent link">&para;</a></h1>
<pre><code class="language-julia">import Zygote
import LinearAlgebra: dot
using MLJ
import MLJBase

function gradientboost(y, X, loss, baselearner; steps=100, stepsize=0.1, linesearch=false)
  F = Vector{Function}(undef, steps)
  F[1] = x-&gt;mean(y)*ones(size(X,1))
  Xtable = MLJBase.table(X)
  if linesearch
    @warn &quot;linesearch=true assumes squared error loss&quot;
  end
  for m ∈ 2:steps
    ∂L∂F=Zygote.gradient(ŷ-&gt;loss(y, ŷ),F[m-1](X))[1]
    basemachine = machine(baselearner, Xtable, ∂L∂F)
    fit!(basemachine)
    ρ = -1.0
    if (linesearch) # assuming least squares
      ỹ = y - F[m-1](X)
      x̃ = predict(basemachine, X)
      ρ = dot(ỹ,x̃)/dot(x̃,x̃)
    end
    F[m] = x-&gt;(F[m-1](x) + stepsize*ρ*predict(basemachine,x))
  end
  return(F)
end

@load DecisionTreeRegressor pkg=BetaML
baselearner = BetaML.Trees.DecisionTreeRegressor(max_depth=2)
X = rand(200, 1)
f(x) = sum(x.^2)
σ = 0.1
y = f.(eachrow(X)) + σ*randn(size(X,1))
loss(Y,Ŷ) = sum( (y-ŷ)^2 for (y, ŷ) ∈ zip(Y, Ŷ) )
F = gradientboost(y,X, loss, baselearner,stepsize=0.1, linesearch=true, steps=100)

using Plots
Plots.pyplot()
scatter(X, y)
p = sortperm(vec(X))
xs = X[p]
plot!(xs, f.(xs))
for m ∈ eachindex(F)
  plot!(xs, F[m](reshape(xs, size(X))), alpha=m/length(F), colour=:black,legend=false)
end
plot!()
</code></pre>
<pre><code>import BetaML ✔
</code></pre>
<p><img alt="" src="figures/gb_2_1.png" /></p>
<h2 id="alternate-baselearner">Alternate baselearner<a class="headerlink" href="#alternate-baselearner" title="Permanent link">&para;</a></h2>
<h1 id="packages">Packages<a class="headerlink" href="#packages" title="Permanent link">&para;</a></h1>
<p>The above code is just for demonstration. It is better to use a package
for gradient boosting. We will focus on gradient boosting trees. The two
leading packages are XGBoost and LightGBM. These packages are written in
C and have Julia (and many other languages) interfaces. LightGBM is
newer, supported by Microsoft, and can be faster. EvoTrees is a pure
Julia implementation and uses some of the same algorithm improvements
that LightGBM does. Benchmarks on the
<a href="https://github.com/Evovest/EvoTrees.jl">EvoTrees</a> github readme show
that it can outperform XGBoost.</p>
<pre><code class="language-julia">#lgbm = @load LGBMRegressor
#xgboost = @load XGBoostRegressor
#evo = @load EvoTreeRegressor
</code></pre>
<h1 id="bibliography-bibliography">Bibliography [bibliography]<a class="headerlink" href="#bibliography-bibliography" title="Permanent link">&para;</a></h1>
<div class="references csl-bib-body hanging-indent" id="refs">
<div class="csl-entry" id="ref-bakhitov2022">
<p>Bakhitov, Edvard, and Amandeep Singh. 2022. “Causal Gradient Boosting:
Boosted Instrumental Variable Regression.” In <em>Proceedings of the 23rd
ACM Conference on Economics and Computation</em>, 604–5. EC ’22. New York,
NY, USA: Association for Computing Machinery.
<a href="https://doi.org/10.1145/3490486.3538251">https://doi.org/10.1145/3490486.3538251</a>.</p>
</div>
<div class="csl-entry" id="ref-bojer2021">
<p>Bojer, Casper Solheim, and Jens Peder Meldgaard. 2021. “Kaggle
Forecasting Competitions: An Overlooked Learning Opportunity.”
<em>International Journal of Forecasting</em> 37 (2): 587–603.
https://doi.org/<a href="https://doi.org/10.1016/j.ijforecast.2020.07.007">https://doi.org/10.1016/j.ijforecast.2020.07.007</a>.</p>
</div>
<div class="csl-entry" id="ref-dombry2021">
<p>Dombry, Clément, and Jean-Jil Duchamps. 2021. “Infinitesimal Gradient
Boosting.”</p>
</div>
<div class="csl-entry" id="ref-friedman2001">
<p>Friedman, Jerome H. 2001. “Greedy Function Approximation: A Gradient
Boosting Machine.” <em>The Annals of Statistics</em> 29 (5): 1189–1232.
<a href="http://www.jstor.org/stable/2699986">http://www.jstor.org/stable/2699986</a>.</p>
</div>
<div class="csl-entry" id="ref-friedman2002">
<p>———. 2002. “Stochastic Gradient Boosting.” <em>Computational Statistics &amp;
Data Analysis</em> 38 (4): 367–78.
https://doi.org/<a href="https://doi.org/10.1016/S0167-9473(01)00065-2">https://doi.org/10.1016/S0167-9473(01)00065-2</a>.</p>
</div>
<div class="csl-entry" id="ref-kueck2017">
<p>Kueck, Jannis, Ye Luo, Martin Spindler, and Zigan Wang. 2017.
“Estimation and Inference of Treatment Effects with $L_2$-Boosting in
High-Dimensional Settings.”</p>
</div>
<div class="csl-entry" id="ref-luo2016">
<p>Luo, Ye, and Martin Spindler. 2016. “High-Dimensional $L_2$Boosting:
Rate of Convergence.”</p>
</div>
<div class="csl-entry" id="ref-m5accuracy">
<p>Makridakis, Spyros, Evangelos Spiliotis, and Vassilios Assimakopoulos.
2022. “M5 Accuracy Competition: Results, Findings, and Conclusions.”
<em>International Journal of Forecasting</em>.
https://doi.org/<a href="https://doi.org/10.1016/j.ijforecast.2021.11.013">https://doi.org/10.1016/j.ijforecast.2021.11.013</a>.</p>
</div>
<div class="csl-entry" id="ref-yang2020">
<p>Yang, Jui-Chung, Hui-Ching Chuang, and Chung-Ming Kuan. 2020. “Double
Machine Learning with Gradient Boosting and Its Application to the Big n
Audit Quality Effect.” <em>Journal of Econometrics</em> 216 (1): 268–83.
https://doi.org/<a href="https://doi.org/10.1016/j.jeconom.2020.01.018">https://doi.org/10.1016/j.jeconom.2020.01.018</a>.</p>
</div>
<div class="csl-entry" id="ref-yue2022">
<p>Yue, Mu, Jialiang Li, and Baoluo Sun. 2022. “Conditional Sparse Boosting
for High-Dimensional Instrumental Variable Estimation.” <em>Journal of
Statistical Computation and Simulation</em> 92 (15): 3087–3108.
<a href="https://doi.org/10.1080/00949655.2022.2056739">https://doi.org/10.1080/00949655.2022.2056739</a>.</p>
</div>
<div class="csl-entry" id="ref-zhang2005">
<p>Zhang, Tong, and Bin Yu. 2005. “<span class="nocase">Boosting with early
stopping: Convergence and consistency</span>.” <em>The Annals of
Statistics</em> 33 (4): 1538–79.
<a href="https://doi.org/10.1214/009053605000000255">https://doi.org/10.1214/009053605000000255</a>.</p>
</div>
<div class="csl-entry" id="ref-zhou2018">
<p>Zhou, Yichen, and Giles Hooker. 2018. “Boulevard: Regularized Stochastic
Gradient Boosted Trees and Their Limiting Distribution.”</p>
</div>
</div></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Paul Schrimpf</p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../assets/mathjaxhelper.js" defer></script>
        <script src="../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
