<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Paul Schrimpf">
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Notes -  </title>
        <link href="../css/bootstrap.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/atelier-forest-light.min.css">
        <link href="../assets/Documenter.css" rel="stylesheet">
        <link href="../assets/extra.css" rel="stylesheet">

        <script src="../js/jquery-1.10.2.min.js" defer></script>
        <script src="../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href=".."> </a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href="../index.md" class="nav-link">Package Docs</a>
                            </li>
                            <li class="navitem active">
                                <a href="./" class="nav-link">Notes</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../license/" class="dropdown-item">License</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" class="nav-link disabled">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../license/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li class="nav-item">
                                <a href="https://github.com/schrimpf/NeuralNetworkEconomics.jl/edit/master/docs/gb.md" class="nav-link"><i class="fa fa-github"></i> Edit on GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#introduction" class="nav-link">Introduction</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#decision-trees" class="nav-link">Decision Trees</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#gradient-boosting" class="nav-link">Gradient Boosting</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#gradient-descent-in-function-space" class="nav-link">Gradient Descent in Function Space</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#gradient-boosting-algorithm" class="nav-link">Gradient Boosting Algorithm</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#regularization" class="nav-link">Regularization</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#statistical-properties" class="nav-link">Statistical Properties</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#example-code" class="nav-link">Example Code</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#packages" class="nav-link">Packages</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#bibliography" class="nav-link">Bibliography</a>
              <ul class="nav flex-column">
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a></p>
<p>This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike
4.0 International
License</a></p>
<p>
<script type="math/tex; mode=display">
\def\indep{\perp\!\!\!\perp}
\def\Er{\mathrm{E}}
\def\R{\mathbb{R}}
\def\En{{\mathbb{E}_n}}
\def\Pr{\mathrm{P}}
\newcommand{\norm}[1]{\left\Vert {#1} \right\Vert}
\newcommand{\abs}[1]{\left\vert {#1} \right\vert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
</script>
</p>
<h1 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h1>
<p>Variants of gradient boosting decision trees often win prediction
contests (e.g. @bojer2021 and @m5accuracy ). @yang2020 examine the
peformance of various estimation methods in double machine learning.
They compare gradient boosted trees, random forests, lasso, neural
networks, and support vector machines, and find that gradient boosting
performs best.</p>
<h1 id="decision-trees">Decision Trees<a class="headerlink" href="#decision-trees" title="Permanent link">&para;</a></h1>
<p>See
<a href="https://schrimpf.github.io/NeuralNetworkEconomics.jl/ml-methods/#random-forests">this.</a></p>
<h1 id="gradient-boosting">Gradient Boosting<a class="headerlink" href="#gradient-boosting" title="Permanent link">&para;</a></h1>
<p>Gradient boosting was first described in detail by @friedman2001, but
the</p>
<p>and its stochastic variant in @friedman2002. Gradient boosting deals
with the following regression or classification problem.</p>
<p>
<script type="math/tex; mode=display">
\min_{F \in mathcal{F}} E_x \left[ E_y\left[ L(y, F(x)) | x \right] \right]
</script>
</p>
<p>where $L$ is some loss function and $\mathcal{F}$ is some family of
functions. For example if</p>
<p>$L$ is sqaured error and $\mathcal{F}$ is polynomials of $x$, then this
problem becomes series regression.</p>
<p>Gradient boosting focuses on the case where $\mathcal{F}$ is an additive
sum of “simple” learners.</p>
<p>
<script type="math/tex; mode=display">
 \mathcal{F} = \{ F(x;\beta, a) = \sum_{m=1}^M \beta_m h(x;a_m)
</script>
</p>
<p>For example, if <script type="math/tex; mode=display">
h(x; a_m) = \psi(x'a_m)
</script> where $\psi$ is some activation function, then $\mathcal{F}$ would be
a single layer neural network of width $M$. Gradient boosting is most
often used when $h(x;a_m)$ is a shallow decision tree.</p>
<p>Fully minimizing over $\beta$ and $a$ can be difficult. Therefore,
gradient boosting computes an approximate minimum through a greedy,
stagewise approach.</p>
<h2 id="gradient-descent-in-function-space">Gradient Descent in Function Space<a class="headerlink" href="#gradient-descent-in-function-space" title="Permanent link">&para;</a></h2>
<p>To motivate gradient boosting, consider what gradient descent in the
original problem looks like. Let
$Q(F) = E_x \left[ E_y\left[ L(y,F(x)) | x \right] \right]$ denote the
objective function. Then gradient descent would first choose an initial
$F_0(m)$, and then:</p>
<ol>
<li>
<p>Calculate <script type="math/tex; mode=display">
    \frac{\partial Q(F_{m-1}}{\partial F} = E_y\left[\frac{\partial L(y, F_{m-1}(x))}{\partial F(x)} | x \right]
    </script>
</p>
</li>
<li>
<p>Set $F_m = F_{m-1} - \nu \frac{\partial Q(F_{m-1}}}{\partial F}$</p>
</li>
<li>
<p>Repeat</p>
</li>
</ol>
<p>Optionally, the step-size, $\nu$ could be adjusted and/or a line search
could be added.</p>
<h2 id="gradient-boosting-algorithm">Gradient Boosting Algorithm<a class="headerlink" href="#gradient-boosting-algorithm" title="Permanent link">&para;</a></h2>
<p>With the constraint that $F(x) = \sum \beta_m h(x; a_m)$, we cannot
always update $F$ in the direction of the gradient. Instead, we choose a
direction that obeys the constraint and is close to the gradient.
Specifically,</p>
<ol>
<li>Choose $a_m$ to approximate the gradient: <script type="math/tex; mode=display">
    a_m = \argmin_a \sum_{i=1}^N \left( \frac{\partial -L(y_i, F_{m-1}(x_i))}{\partial F(x)} - h(x_i; a) \right)^2
    </script>
</li>
<li>Line search: <script type="math/tex; mode=display">
    \rho_m = \argmin_\rho \sum_{i=1}^N L\left( y_i, F_{m-1}(x_i) + \rho h(x_i;a_m) \right)
    </script>
</li>
<li>Update: <script type="math/tex; mode=display">
    F_m(x) = F_{m-1}(x) + \nu \rho_m h(x; a_m)
    </script>
</li>
<li>Repeat</li>
</ol>
<p>Notice that when $L$ is squared error loss, then step 1 becomes fitting
$h(x_i;a)$ to the residuals. This idea of updating a model by fitting
the residuals existed before @friedman2001 , and is where the name
“boosting” originates.</p>
<h2 id="regularization">Regularization<a class="headerlink" href="#regularization" title="Permanent link">&para;</a></h2>
<p>There are at least three parameters that affect model complexity: the
number of iterations, $M$, the step size / shrinkage parameter, $\nu$,
and the complexity of $h(x,a)$.</p>
<p>In the case of trees, the depth of $h(x,a)$ has a clear implication for
type of functions that the model can well approximate. If $h(x,a)$ are
stubs – trees with a single split, then each $h(x,a)$ is a function of
only one component of $x$. Therefore we should expect to be able to
approximate additively separable functions like</p>
<p>$$
f_1(x_1) + f_2(x_2) + &hellip; + f_k(x_k)$
$$</p>
<p>well. Trees with two splits can accomodate pairwise interactions, etc.</p>
<h2 id="statistical-properties">Statistical Properties<a class="headerlink" href="#statistical-properties" title="Permanent link">&para;</a></h2>
<p>@zhang2005 has consistency and convergence rate results. Whether those
results are compatible with double machine learning is not yet clear to
me.</p>
<p>@zhou2018 also appears relevant. It gives pointwise asymptotic
normality. Like Athey et al’s results for forests, the class of
functions considered is too rich to get fast convergence rates.</p>
<p>@dombry2021 ODE view of boosting</p>
<p>In high dimensional settings (i.e. $x$ is high dimensional) there is a
relationship between boosting and lasso, see @luo2016 and @kueck2017.</p>
<p>Note: A creative definition of $x$ as indicators that your base tree can
generate might make @luo2016 results applicable to boosting trees. The
key would be verifying the sparse eigenvalue condition A.2, and the
restriction that the “true” function has a sparse representation in
terms of these indicators.</p>
<h1 id="example-code">Example Code<a class="headerlink" href="#example-code" title="Permanent link">&para;</a></h1>
<pre><code class="language-julia">import Zygote
import LinearAlgebra: dot

function gradientboost(y, X, loss, baselearner; steps=100, stepsize=0.1, linesearch=false)
  F = Vector{Function}(undef, steps)
  F[1] = x-&gt;mean(y)*ones(size(X,1))
  Xtable = MLJBase.table(X)
  if linesearch
    @warn &quot;linesearch=true assumes squared error loss&quot;
  end
  for m ∈ 2:steps
    ∂L∂F=Zygote.gradient(ŷ-&gt;loss(y, ŷ),F[m-1](X))[1]
    basemachine = machine(baselearner, Xtable, ∂L∂F)
    fit!(basemachine)
    ρ = -1.0
    if (linesearch) # assuming least squares
      ỹ = y - F[m-1](X)
      x̃ = predict(basemachine, X)
      ρ = dot(ỹ,x̃)/dot(x̃,x̃)
    end
    F[m] = x-&gt;(F[m-1](x) + stepsize*ρ*predict(basemachine,x))
  end
  return(F)
end

@load DecisionTreeRegressor pkg=BetaML
baselearner = BetaML.Trees.DecisionTreeRegressor(maxDepth=2)
X = rand(200, 1)
f(x) = sum(x.^2)
σ = 0.1
y = f.(eachrow(X)) + σ*randn(size(X,1))
loss(Y,Ŷ) = sum( (y-ŷ)^2 for (y, ŷ) ∈ zip(Y, Ŷ) )
F = gradientboost(y,X, loss, baselearner,stepsize=0.1, linesearch=true, steps=100)

using Plots
scatter(X, y)
p = sortperm(vec(X))
xs = X[p]
plot!(xs, f.(xs))
for m ∈ eachindex(F)
  plot!(xs, F[m](reshape(xs, size(X))), alpha=m/length(F), colour=:black,legend=false)
end
</code></pre>
<pre><code>Error: ArgumentError: Package Zygote not found in current path:
- Run `import Pkg; Pkg.add("Zygote")` to install the Zygote package.
</code></pre>
<h1 id="packages">Packages<a class="headerlink" href="#packages" title="Permanent link">&para;</a></h1>
<p>The above code is just for demonstration. It is better to use a package
for gradient boosting. We will focus on gradient boosting trees. The two
leading packages are XGBoost and LightGBM. These packages are written in
C and have Julia (and many other languages) interfaces. LightGBM is
newer, supported by Microsoft, and can be faster. EvoTrees is a pure
Julia implementation and uses some of the same algorithm improvements
that LightGBM does. Benchmarks on the
<a href="https://github.com/Evovest/EvoTrees.jl">EvoTrees</a> github readme show
that it can outperform XGBoost.</p>
<pre><code class="language-julia">@load LGBMRegressor
@load XGBoostRegressor
@load EvoTreeRegressor
</code></pre>
<pre><code>Error: LoadError: UndefVarError: @load not defined
in expression starting at /home/paul/.julia/dev/GradientBoostingNotes/docs/
jmd/gb.jmd:2
</code></pre>
<h1 id="bibliography">Bibliography<a class="headerlink" href="#bibliography" title="Permanent link">&para;</a></h1></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Paul Schrimpf</p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../assets/mathjaxhelper.js" defer></script>
        <script src="../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
