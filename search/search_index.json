{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"gb/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} Introduction \u00b6 Variants of gradient boosting decision trees often win prediction contests (e.g. @bojer2021 and @m5accuracy ). @yang2020 examine the peformance of various estimation methods in double machine learning. They compare gradient boosted trees, random forests, lasso, neural networks, and support vector machines, and find that gradient boosting performs best. Decision Trees \u00b6 See this. Gradient Boosting \u00b6 Gradient boosting was first described in detail by @friedman2001, but the and its stochastic variant in @friedman2002. Gradient boosting deals with the following regression or classification problem. \\min_{F \\in mathcal{F}} E_x \\left[ E_y\\left[ L(y, F(x)) | x \\right] \\right] where $L$ is some loss function and $\\mathcal{F}$ is some family of functions. For example if $L$ is sqaured error and $\\mathcal{F}$ is polynomials of $x$, then this problem becomes series regression. Gradient boosting focuses on the case where $\\mathcal{F}$ is an additive sum of \u201csimple\u201d learners. \\mathcal{F} = \\{ F(x;\\beta, a) = \\sum_{m=1}^M \\beta_m h(x;a_m) For example, if h(x; a_m) = \\psi(x'a_m) where $\\psi$ is some activation function, then $\\mathcal{F}$ would be a single layer neural network of width $M$. Gradient boosting is most often used when $h(x;a_m)$ is a shallow decision tree. Fully minimizing over $\\beta$ and $a$ can be difficult. Therefore, gradient boosting computes an approximate minimum through a greedy, stagewise approach. Gradient Descent in Function Space \u00b6 To motivate gradient boosting, consider what gradient descent in the original problem looks like. Let $Q(F) = E_x \\left[ E_y\\left[ L(y,F(x)) | x \\right] \\right]$ denote the objective function. Then gradient descent would first choose an initial $F_0(m)$, and then: Calculate \\frac{\\partial Q(F_{m-1}}{\\partial F} = E_y\\left[\\frac{\\partial L(y, F_{m-1}(x))}{\\partial F(x)} | x \\right] Set $F_m = F_{m-1} - \\nu \\frac{\\partial Q(F_{m-1}}}{\\partial F}$ Repeat Optionally, the step-size, $\\nu$ could be adjusted and/or a line search could be added. Gradient Boosting Algorithm \u00b6 With the constraint that $F(x) = \\sum \\beta_m h(x; a_m)$, we cannot always update $F$ in the direction of the gradient. Instead, we choose a direction that obeys the constraint and is close to the gradient. Specifically, Choose $a_m$ to approximate the gradient: a_m = \\argmin_a \\sum_{i=1}^N \\left( \\frac{\\partial -L(y_i, F_{m-1}(x_i))}{\\partial F(x)} - h(x_i; a) \\right)^2 Line search: \\rho_m = \\argmin_\\rho \\sum_{i=1}^N L\\left( y_i, F_{m-1}(x_i) + \\rho h(x_i;a_m) \\right) Update: F_m(x) = F_{m-1}(x) + \\nu \\rho_m h(x; a_m) Repeat Notice that when $L$ is squared error loss, then step 1 becomes fitting $h(x_i;a)$ to the residuals. This idea of updating a model by fitting the residuals existed before @friedman2001 , and is where the name \u201cboosting\u201d originates. Regularization \u00b6 There are at least three parameters that affect model complexity: the number of iterations, $M$, the step size / shrinkage parameter, $\\nu$, and the complexity of $h(x,a)$. In the case of trees, the depth of $h(x,a)$ has a clear implication for type of functions that the model can well approximate. If $h(x,a)$ are stubs \u2013 trees with a single split, then each $h(x,a)$ is a function of only one component of $x$. Therefore we should expect to be able to approximate additively separable functions like $$ f_1(x_1) + f_2(x_2) + \u2026 + f_k(x_k)$ $$ well. Trees with two splits can accomodate pairwise interactions, etc. Statistical Properties \u00b6 @zhang2005 has consistency and convergence rate results. Whether those results are compatible with double machine learning is not yet clear to me. @zhou2018 also appears relevant. It gives pointwise asymptotic normality. Like Athey et al\u2019s results for forests, the class of functions considered is too rich to get fast convergence rates. @dombry2021 ODE view of boosting @yang2020 has simulation evidence that gradient boosting outperforms other methods in double machine learning, but not formal proofs. In high dimensional settings (i.e. $x$ is high dimensional) there is a relationship between boosting and lasso, see @luo2016 and @kueck2017. Note: A creative definition of $x$ as indicators that your base tree can generate might make @luo2016 results applicable to boosting trees. The key would be verifying the sparse eigenvalue condition A.2, and the restriction that the \u201ctrue\u201d function has a sparse representation in terms of these indicators. Example Code \u00b6 import Zygote import LinearAlgebra: dot using MLJ import MLJBase function gradientboost(y, X, loss, baselearner; steps=100, stepsize=0.1, linesearch=false) F = Vector{Function}(undef, steps) F[1] = x->mean(y)*ones(size(X,1)) Xtable = MLJBase.table(X) if linesearch @warn \"linesearch=true assumes squared error loss\" end for m \u2208 2:steps \u2202L\u2202F=Zygote.gradient(y\u0302->loss(y, y\u0302),F[m-1](X))[1] basemachine = machine(baselearner, Xtable, \u2202L\u2202F) fit!(basemachine) \u03c1 = -1.0 if (linesearch) # assuming least squares y\u0303 = y - F[m-1](X) x\u0303 = predict(basemachine, X) \u03c1 = dot(y\u0303,x\u0303)/dot(x\u0303,x\u0303) end F[m] = x->(F[m-1](x) + stepsize*\u03c1*predict(basemachine,x)) end return(F) end @load DecisionTreeRegressor pkg=BetaML baselearner = BetaML.Trees.DecisionTreeRegressor(max_depth=2) X = rand(200, 1) f(x) = sum(x.^2) \u03c3 = 0.1 y = f.(eachrow(X)) + \u03c3*randn(size(X,1)) loss(Y,Y\u0302) = sum( (y-y\u0302)^2 for (y, y\u0302) \u2208 zip(Y, Y\u0302) ) F = gradientboost(y,X, loss, baselearner,stepsize=0.1, linesearch=true, steps=100) using Plots scatter(X, y) p = sortperm(vec(X)) xs = X[p] plot!(xs, f.(xs)) for m \u2208 eachindex(F) plot!(xs, F[m](reshape(xs, size(X))), alpha=m/length(F), colour=:black,legend=false) end function showplot(m) scatter(X, y) p = sortperm(vec(X)) xs = X[p] plot!(xs, f.(xs)) plot!(xs, F[m](reshape(xs, size(X))), alpha=m/length(F), colour=:black,legend=false) end import BetaML \u2714 showplot (generic function with 1 method) Packages \u00b6 The above code is just for demonstration. It is better to use a package for gradient boosting. We will focus on gradient boosting trees. The two leading packages are XGBoost and LightGBM. These packages are written in C and have Julia (and many other languages) interfaces. LightGBM is newer, supported by Microsoft, and can be faster. EvoTrees is a pure Julia implementation and uses some of the same algorithm improvements that LightGBM does. Benchmarks on the EvoTrees github readme show that it can outperform XGBoost. lgbm = @load LGBMRegressor xgboost = @load XGBoostRegressor evo = @load EvoTreeRegressor import LightGBM \u2714 import MLJXGBoostInterface \u2714 import EvoTrees \u2714 EvoTrees.EvoTreeRegressor Bibliography \u00b6","title":"Notes"},{"location":"gb/#introduction","text":"Variants of gradient boosting decision trees often win prediction contests (e.g. @bojer2021 and @m5accuracy ). @yang2020 examine the peformance of various estimation methods in double machine learning. They compare gradient boosted trees, random forests, lasso, neural networks, and support vector machines, and find that gradient boosting performs best.","title":"Introduction"},{"location":"gb/#decision-trees","text":"See this.","title":"Decision Trees"},{"location":"gb/#gradient-boosting","text":"Gradient boosting was first described in detail by @friedman2001, but the and its stochastic variant in @friedman2002. Gradient boosting deals with the following regression or classification problem. \\min_{F \\in mathcal{F}} E_x \\left[ E_y\\left[ L(y, F(x)) | x \\right] \\right] where $L$ is some loss function and $\\mathcal{F}$ is some family of functions. For example if $L$ is sqaured error and $\\mathcal{F}$ is polynomials of $x$, then this problem becomes series regression. Gradient boosting focuses on the case where $\\mathcal{F}$ is an additive sum of \u201csimple\u201d learners. \\mathcal{F} = \\{ F(x;\\beta, a) = \\sum_{m=1}^M \\beta_m h(x;a_m) For example, if h(x; a_m) = \\psi(x'a_m) where $\\psi$ is some activation function, then $\\mathcal{F}$ would be a single layer neural network of width $M$. Gradient boosting is most often used when $h(x;a_m)$ is a shallow decision tree. Fully minimizing over $\\beta$ and $a$ can be difficult. Therefore, gradient boosting computes an approximate minimum through a greedy, stagewise approach.","title":"Gradient Boosting"},{"location":"gb/#gradient-descent-in-function-space","text":"To motivate gradient boosting, consider what gradient descent in the original problem looks like. Let $Q(F) = E_x \\left[ E_y\\left[ L(y,F(x)) | x \\right] \\right]$ denote the objective function. Then gradient descent would first choose an initial $F_0(m)$, and then: Calculate \\frac{\\partial Q(F_{m-1}}{\\partial F} = E_y\\left[\\frac{\\partial L(y, F_{m-1}(x))}{\\partial F(x)} | x \\right] Set $F_m = F_{m-1} - \\nu \\frac{\\partial Q(F_{m-1}}}{\\partial F}$ Repeat Optionally, the step-size, $\\nu$ could be adjusted and/or a line search could be added.","title":"Gradient Descent in Function Space"},{"location":"gb/#gradient-boosting-algorithm","text":"With the constraint that $F(x) = \\sum \\beta_m h(x; a_m)$, we cannot always update $F$ in the direction of the gradient. Instead, we choose a direction that obeys the constraint and is close to the gradient. Specifically, Choose $a_m$ to approximate the gradient: a_m = \\argmin_a \\sum_{i=1}^N \\left( \\frac{\\partial -L(y_i, F_{m-1}(x_i))}{\\partial F(x)} - h(x_i; a) \\right)^2 Line search: \\rho_m = \\argmin_\\rho \\sum_{i=1}^N L\\left( y_i, F_{m-1}(x_i) + \\rho h(x_i;a_m) \\right) Update: F_m(x) = F_{m-1}(x) + \\nu \\rho_m h(x; a_m) Repeat Notice that when $L$ is squared error loss, then step 1 becomes fitting $h(x_i;a)$ to the residuals. This idea of updating a model by fitting the residuals existed before @friedman2001 , and is where the name \u201cboosting\u201d originates.","title":"Gradient Boosting Algorithm"},{"location":"gb/#regularization","text":"There are at least three parameters that affect model complexity: the number of iterations, $M$, the step size / shrinkage parameter, $\\nu$, and the complexity of $h(x,a)$. In the case of trees, the depth of $h(x,a)$ has a clear implication for type of functions that the model can well approximate. If $h(x,a)$ are stubs \u2013 trees with a single split, then each $h(x,a)$ is a function of only one component of $x$. Therefore we should expect to be able to approximate additively separable functions like $$ f_1(x_1) + f_2(x_2) + \u2026 + f_k(x_k)$ $$ well. Trees with two splits can accomodate pairwise interactions, etc.","title":"Regularization"},{"location":"gb/#statistical-properties","text":"@zhang2005 has consistency and convergence rate results. Whether those results are compatible with double machine learning is not yet clear to me. @zhou2018 also appears relevant. It gives pointwise asymptotic normality. Like Athey et al\u2019s results for forests, the class of functions considered is too rich to get fast convergence rates. @dombry2021 ODE view of boosting @yang2020 has simulation evidence that gradient boosting outperforms other methods in double machine learning, but not formal proofs. In high dimensional settings (i.e. $x$ is high dimensional) there is a relationship between boosting and lasso, see @luo2016 and @kueck2017. Note: A creative definition of $x$ as indicators that your base tree can generate might make @luo2016 results applicable to boosting trees. The key would be verifying the sparse eigenvalue condition A.2, and the restriction that the \u201ctrue\u201d function has a sparse representation in terms of these indicators.","title":"Statistical Properties"},{"location":"gb/#example-code","text":"import Zygote import LinearAlgebra: dot using MLJ import MLJBase function gradientboost(y, X, loss, baselearner; steps=100, stepsize=0.1, linesearch=false) F = Vector{Function}(undef, steps) F[1] = x->mean(y)*ones(size(X,1)) Xtable = MLJBase.table(X) if linesearch @warn \"linesearch=true assumes squared error loss\" end for m \u2208 2:steps \u2202L\u2202F=Zygote.gradient(y\u0302->loss(y, y\u0302),F[m-1](X))[1] basemachine = machine(baselearner, Xtable, \u2202L\u2202F) fit!(basemachine) \u03c1 = -1.0 if (linesearch) # assuming least squares y\u0303 = y - F[m-1](X) x\u0303 = predict(basemachine, X) \u03c1 = dot(y\u0303,x\u0303)/dot(x\u0303,x\u0303) end F[m] = x->(F[m-1](x) + stepsize*\u03c1*predict(basemachine,x)) end return(F) end @load DecisionTreeRegressor pkg=BetaML baselearner = BetaML.Trees.DecisionTreeRegressor(max_depth=2) X = rand(200, 1) f(x) = sum(x.^2) \u03c3 = 0.1 y = f.(eachrow(X)) + \u03c3*randn(size(X,1)) loss(Y,Y\u0302) = sum( (y-y\u0302)^2 for (y, y\u0302) \u2208 zip(Y, Y\u0302) ) F = gradientboost(y,X, loss, baselearner,stepsize=0.1, linesearch=true, steps=100) using Plots scatter(X, y) p = sortperm(vec(X)) xs = X[p] plot!(xs, f.(xs)) for m \u2208 eachindex(F) plot!(xs, F[m](reshape(xs, size(X))), alpha=m/length(F), colour=:black,legend=false) end function showplot(m) scatter(X, y) p = sortperm(vec(X)) xs = X[p] plot!(xs, f.(xs)) plot!(xs, F[m](reshape(xs, size(X))), alpha=m/length(F), colour=:black,legend=false) end import BetaML \u2714 showplot (generic function with 1 method)","title":"Example Code"},{"location":"gb/#packages","text":"The above code is just for demonstration. It is better to use a package for gradient boosting. We will focus on gradient boosting trees. The two leading packages are XGBoost and LightGBM. These packages are written in C and have Julia (and many other languages) interfaces. LightGBM is newer, supported by Microsoft, and can be faster. EvoTrees is a pure Julia implementation and uses some of the same algorithm improvements that LightGBM does. Benchmarks on the EvoTrees github readme show that it can outperform XGBoost. lgbm = @load LGBMRegressor xgboost = @load XGBoostRegressor evo = @load EvoTreeRegressor import LightGBM \u2714 import MLJXGBoostInterface \u2714 import EvoTrees \u2714 EvoTrees.EvoTreeRegressor","title":"Packages"},{"location":"gb/#bibliography","text":"","title":"Bibliography"},{"location":"license/","text":"The notes are licensed under a Creative Commons Attribution-ShareAlike 4.0 International License and were written by Paul Schrimpf.","title":"License"}]}