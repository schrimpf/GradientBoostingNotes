{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"gb/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} Introduction \u00b6 Variants of gradient boosting decision trees often win prediction contests (e.g. Bojer and Meldgaard ( 2021 ) and Makridakis, Spiliotis, and Assimakopoulos ( 2022 ) ). Yang, Chuang, and Kuan ( 2020 ) examine the peformance of various estimation methods in double machine learning. They compare gradient boosted trees, random forests, lasso, neural networks, and support vector machines, and find that gradient boosting performs best. Decision Trees \u00b6 See this. Gradient Boosting \u00b6 Gradient boosting was first described in detail by Friedman ( 2001 ), and its stochastic variant in Friedman ( 2002 ). Gradient boosting deals with the following regression or classification problem. \\min_{F \\in \\mathcal{F}} E_x \\left[ E_y\\left[ L(y, F(x)) | x \\right] \\right] where $L$ is some loss function and $\\mathcal{F}$ is some family of functions. For example if $L$ is sqaured error and $\\mathcal{F}$ is polynomials of $x$, then this problem becomes series regression. Gradient boosting focuses on the case where $\\mathcal{F}$ is an additive sum of \u201csimple\u201d learners. \\mathcal{F} = \\{ F(x;\\beta, a) = \\sum_{m=1}^M \\beta_m h(x;a_m) \\} For example, if h(x; a_m) = \\psi(x'a_m) where $\\psi$ is some activation function, then $\\mathcal{F}$ would be a single layer neural network of width $M$. Gradient boosting is most often used when $h(x;a_m)$ is a shallow decision tree. Fully minimizing over $\\beta$ and $a$ can be difficult. Therefore, gradient boosting computes an approximate minimum through a greedy, stagewise approach. Gradient Descent in Function Space \u00b6 To motivate gradient boosting, consider what gradient descent in the original problem looks like. Let $Q(F) = E_x \\left[ E_y\\left[ L(y,F(x)) | x \\right] \\right]$ denote the objective function. Then gradient descent would first choose an initial $F_0(m)$, and then: Calculate \\frac{\\partial Q(F_{m-1}}{\\partial F} = E_y\\left[\\frac{\\partial L(y, F_{m-1}(x))}{\\partial F(x)} | x \\right] Set $F_m = F_{m-1} - \\nu \\frac{\\partial Q(F_{m-1})}{\\partial F}$ Repeat Optionally, the step-size, $\\nu$ could be adjusted and/or a line search could be added. Gradient Boosting Algorithm \u00b6 With the constraint that $F(x) = \\sum \\beta_m h(x; a_m)$, we cannot always update $F$ in the direction of the gradient. Instead, we choose a direction that obeys the constraint and is close to the gradient. Specifically, Choose $a_m$ to approximate the gradient: a_m = \\textrm{arg}\\min_a \\sum_{i=1}^N \\left( \\frac{\\partial -L(y_i, F_{m-1}(x_i))}{\\partial F(x)} - h(x_i; a) \\right)^2 Line search: \\rho_m = \\textrm{arg}\\min_\\rho \\sum_{i=1}^N L\\left( y_i, F_{m-1}(x_i) + \\rho h(x_i;a_m) \\right) Update: F_m(x) = F_{m-1}(x) + \\nu \\rho_m h(x; a_m) Repeat Notice that when $L$ is squared error loss, then step 1 becomes fitting $h(x_i;a)$ to the residuals. This idea of updating a model by fitting the residuals existed before Friedman ( 2001 ) , and is where the name \u201cboosting\u201d originates. Regularization \u00b6 There are at least three parameters that affect model complexity: the number of iterations, $M$, the step size / shrinkage parameter, $\\nu$, and the complexity of $h(x,a)$. In the case of trees, the depth of $h(x,a)$ has a clear implication for type of functions that the model can well approximate. If $h(x,a)$ are stubs \u2013 trees with a single split, then each $h(x,a)$ is a function of only one component of $x$. Therefore we should expect to be able to approximate additively separable functions like f_1(x_1) + f_2(x_2) + ... + f_k(x_k) well. Trees with two splits can accomodate pairwise interactions, etc. Statistical Properties and References \u00b6 Zhang and Yu ( 2005 ) has consistency and convergence rate results. Whether those results are compatible with double machine learning is not yet clear to me. Zhou and Hooker ( 2018 ) also appears relevant. It gives pointwise asymptotic normality. Like Athey et al\u2019s results for forests, the class of functions considered is too rich to get fast convergence rates. Dombry and Duchamps ( 2021 ) ODE view of boosting Yang, Chuang, and Kuan ( 2020 ) has simulation evidence that gradient boosting outperforms other methods in double machine learning, but not formal proofs. In high dimensional settings (i.e. $x$ is high dimensional, and $h(x,a_m)$ is selecting one covariate) there is a relationship between boosting and lasso, see Luo and Spindler ( 2016 ) and Kueck et al. ( 2017 ). Note: A creative definition of $x$ as indicators that your base tree can generate might make Luo and Spindler ( 2016 ) results applicable to boosting trees. The key would be verifying the sparse eigenvalue condition A.2, and the restriction that the \u201ctrue\u201d function has a sparse representation in terms of these indicators. Yue, Li, and Sun ( 2022 ) and Bakhitov and Singh ( 2022 ) combine boosting with instrumental variables. Example Code \u00b6 import Zygote import LinearAlgebra: dot using MLJ import MLJBase function gradientboost(y, X, loss, baselearner; steps=100, stepsize=0.1, linesearch=false) F = Vector{Function}(undef, steps) F[1] = x->mean(y)*ones(size(X,1)) Xtable = MLJBase.table(X) if linesearch @warn \"linesearch=true assumes squared error loss\" end for m \u2208 2:steps \u2202L\u2202F=Zygote.gradient(y\u0302->loss(y, y\u0302),F[m-1](X))[1] basemachine = machine(baselearner, Xtable, \u2202L\u2202F) fit!(basemachine) \u03c1 = -1.0 if (linesearch) # assuming least squares y\u0303 = y - F[m-1](X) x\u0303 = predict(basemachine, X) \u03c1 = dot(y\u0303,x\u0303)/dot(x\u0303,x\u0303) end F[m] = x->(F[m-1](x) + stepsize*\u03c1*predict(basemachine,x)) end return(F) end @load DecisionTreeRegressor pkg=BetaML baselearner = BetaML.Trees.DecisionTreeRegressor(max_depth=2) X = rand(200, 1) f(x) = sum(x.^2) \u03c3 = 0.1 y = f.(eachrow(X)) + \u03c3*randn(size(X,1)) loss(Y,Y\u0302) = sum( (y-y\u0302)^2 for (y, y\u0302) \u2208 zip(Y, Y\u0302) ) F = gradientboost(y,X, loss, baselearner,stepsize=0.1, linesearch=true, steps=100) using Plots Plots.pyplot() scatter(X, y) p = sortperm(vec(X)) xs = X[p] plot!(xs, f.(xs)) for m \u2208 eachindex(F) plot!(xs, F[m](reshape(xs, size(X))), alpha=m/length(F), colour=:black,legend=false) end plot!() import BetaML \u2714 Alternate baselearner \u00b6 Packages \u00b6 The above code is just for demonstration. It is better to use a package for gradient boosting. We will focus on gradient boosting trees. The two leading packages are XGBoost and LightGBM. These packages are written in C and have Julia (and many other languages) interfaces. LightGBM is newer, supported by Microsoft, and can be faster. EvoTrees is a pure Julia implementation and uses some of the same algorithm improvements that LightGBM does. Benchmarks on the EvoTrees github readme show that it can outperform XGBoost. #lgbm = @load LGBMRegressor #xgboost = @load XGBoostRegressor #evo = @load EvoTreeRegressor Bibliography [bibliography] \u00b6 Bakhitov, Edvard, and Amandeep Singh. 2022. \u201cCausal Gradient Boosting: Boosted Instrumental Variable Regression.\u201d In Proceedings of the 23rd ACM Conference on Economics and Computation , 604\u20135. EC \u201922. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3490486.3538251 . Bojer, Casper Solheim, and Jens Peder Meldgaard. 2021. \u201cKaggle Forecasting Competitions: An Overlooked Learning Opportunity.\u201d International Journal of Forecasting 37 (2): 587\u2013603. https://doi.org/ https://doi.org/10.1016/j.ijforecast.2020.07.007 . Dombry, Cl\u00e9ment, and Jean-Jil Duchamps. 2021. \u201cInfinitesimal Gradient Boosting.\u201d Friedman, Jerome H. 2001. \u201cGreedy Function Approximation: A Gradient Boosting Machine.\u201d The Annals of Statistics 29 (5): 1189\u20131232. http://www.jstor.org/stable/2699986 . \u2014\u2014\u2014. 2002. \u201cStochastic Gradient Boosting.\u201d Computational Statistics & Data Analysis 38 (4): 367\u201378. https://doi.org/ https://doi.org/10.1016/S0167-9473(01)00065-2 . Kueck, Jannis, Ye Luo, Martin Spindler, and Zigan Wang. 2017. \u201cEstimation and Inference of Treatment Effects with $L_2$-Boosting in High-Dimensional Settings.\u201d Luo, Ye, and Martin Spindler. 2016. \u201cHigh-Dimensional $L_2$Boosting: Rate of Convergence.\u201d Makridakis, Spyros, Evangelos Spiliotis, and Vassilios Assimakopoulos. 2022. \u201cM5 Accuracy Competition: Results, Findings, and Conclusions.\u201d International Journal of Forecasting . https://doi.org/ https://doi.org/10.1016/j.ijforecast.2021.11.013 . Yang, Jui-Chung, Hui-Ching Chuang, and Chung-Ming Kuan. 2020. \u201cDouble Machine Learning with Gradient Boosting and Its Application to the Big n Audit Quality Effect.\u201d Journal of Econometrics 216 (1): 268\u201383. https://doi.org/ https://doi.org/10.1016/j.jeconom.2020.01.018 . Yue, Mu, Jialiang Li, and Baoluo Sun. 2022. \u201cConditional Sparse Boosting for High-Dimensional Instrumental Variable Estimation.\u201d Journal of Statistical Computation and Simulation 92 (15): 3087\u20133108. https://doi.org/10.1080/00949655.2022.2056739 . Zhang, Tong, and Bin Yu. 2005. \u201c Boosting with early stopping: Convergence and consistency .\u201d The Annals of Statistics 33 (4): 1538\u201379. https://doi.org/10.1214/009053605000000255 . Zhou, Yichen, and Giles Hooker. 2018. \u201cBoulevard: Regularized Stochastic Gradient Boosted Trees and Their Limiting Distribution.\u201d","title":"Notes"},{"location":"gb/#introduction","text":"Variants of gradient boosting decision trees often win prediction contests (e.g. Bojer and Meldgaard ( 2021 ) and Makridakis, Spiliotis, and Assimakopoulos ( 2022 ) ). Yang, Chuang, and Kuan ( 2020 ) examine the peformance of various estimation methods in double machine learning. They compare gradient boosted trees, random forests, lasso, neural networks, and support vector machines, and find that gradient boosting performs best.","title":"Introduction"},{"location":"gb/#decision-trees","text":"See this.","title":"Decision Trees"},{"location":"gb/#gradient-boosting","text":"Gradient boosting was first described in detail by Friedman ( 2001 ), and its stochastic variant in Friedman ( 2002 ). Gradient boosting deals with the following regression or classification problem. \\min_{F \\in \\mathcal{F}} E_x \\left[ E_y\\left[ L(y, F(x)) | x \\right] \\right] where $L$ is some loss function and $\\mathcal{F}$ is some family of functions. For example if $L$ is sqaured error and $\\mathcal{F}$ is polynomials of $x$, then this problem becomes series regression. Gradient boosting focuses on the case where $\\mathcal{F}$ is an additive sum of \u201csimple\u201d learners. \\mathcal{F} = \\{ F(x;\\beta, a) = \\sum_{m=1}^M \\beta_m h(x;a_m) \\} For example, if h(x; a_m) = \\psi(x'a_m) where $\\psi$ is some activation function, then $\\mathcal{F}$ would be a single layer neural network of width $M$. Gradient boosting is most often used when $h(x;a_m)$ is a shallow decision tree. Fully minimizing over $\\beta$ and $a$ can be difficult. Therefore, gradient boosting computes an approximate minimum through a greedy, stagewise approach.","title":"Gradient Boosting"},{"location":"gb/#gradient-descent-in-function-space","text":"To motivate gradient boosting, consider what gradient descent in the original problem looks like. Let $Q(F) = E_x \\left[ E_y\\left[ L(y,F(x)) | x \\right] \\right]$ denote the objective function. Then gradient descent would first choose an initial $F_0(m)$, and then: Calculate \\frac{\\partial Q(F_{m-1}}{\\partial F} = E_y\\left[\\frac{\\partial L(y, F_{m-1}(x))}{\\partial F(x)} | x \\right] Set $F_m = F_{m-1} - \\nu \\frac{\\partial Q(F_{m-1})}{\\partial F}$ Repeat Optionally, the step-size, $\\nu$ could be adjusted and/or a line search could be added.","title":"Gradient Descent in Function Space"},{"location":"gb/#gradient-boosting-algorithm","text":"With the constraint that $F(x) = \\sum \\beta_m h(x; a_m)$, we cannot always update $F$ in the direction of the gradient. Instead, we choose a direction that obeys the constraint and is close to the gradient. Specifically, Choose $a_m$ to approximate the gradient: a_m = \\textrm{arg}\\min_a \\sum_{i=1}^N \\left( \\frac{\\partial -L(y_i, F_{m-1}(x_i))}{\\partial F(x)} - h(x_i; a) \\right)^2 Line search: \\rho_m = \\textrm{arg}\\min_\\rho \\sum_{i=1}^N L\\left( y_i, F_{m-1}(x_i) + \\rho h(x_i;a_m) \\right) Update: F_m(x) = F_{m-1}(x) + \\nu \\rho_m h(x; a_m) Repeat Notice that when $L$ is squared error loss, then step 1 becomes fitting $h(x_i;a)$ to the residuals. This idea of updating a model by fitting the residuals existed before Friedman ( 2001 ) , and is where the name \u201cboosting\u201d originates.","title":"Gradient Boosting Algorithm"},{"location":"gb/#regularization","text":"There are at least three parameters that affect model complexity: the number of iterations, $M$, the step size / shrinkage parameter, $\\nu$, and the complexity of $h(x,a)$. In the case of trees, the depth of $h(x,a)$ has a clear implication for type of functions that the model can well approximate. If $h(x,a)$ are stubs \u2013 trees with a single split, then each $h(x,a)$ is a function of only one component of $x$. Therefore we should expect to be able to approximate additively separable functions like f_1(x_1) + f_2(x_2) + ... + f_k(x_k) well. Trees with two splits can accomodate pairwise interactions, etc.","title":"Regularization"},{"location":"gb/#statistical-properties-and-references","text":"Zhang and Yu ( 2005 ) has consistency and convergence rate results. Whether those results are compatible with double machine learning is not yet clear to me. Zhou and Hooker ( 2018 ) also appears relevant. It gives pointwise asymptotic normality. Like Athey et al\u2019s results for forests, the class of functions considered is too rich to get fast convergence rates. Dombry and Duchamps ( 2021 ) ODE view of boosting Yang, Chuang, and Kuan ( 2020 ) has simulation evidence that gradient boosting outperforms other methods in double machine learning, but not formal proofs. In high dimensional settings (i.e. $x$ is high dimensional, and $h(x,a_m)$ is selecting one covariate) there is a relationship between boosting and lasso, see Luo and Spindler ( 2016 ) and Kueck et al. ( 2017 ). Note: A creative definition of $x$ as indicators that your base tree can generate might make Luo and Spindler ( 2016 ) results applicable to boosting trees. The key would be verifying the sparse eigenvalue condition A.2, and the restriction that the \u201ctrue\u201d function has a sparse representation in terms of these indicators. Yue, Li, and Sun ( 2022 ) and Bakhitov and Singh ( 2022 ) combine boosting with instrumental variables.","title":"Statistical Properties and References"},{"location":"gb/#example-code","text":"import Zygote import LinearAlgebra: dot using MLJ import MLJBase function gradientboost(y, X, loss, baselearner; steps=100, stepsize=0.1, linesearch=false) F = Vector{Function}(undef, steps) F[1] = x->mean(y)*ones(size(X,1)) Xtable = MLJBase.table(X) if linesearch @warn \"linesearch=true assumes squared error loss\" end for m \u2208 2:steps \u2202L\u2202F=Zygote.gradient(y\u0302->loss(y, y\u0302),F[m-1](X))[1] basemachine = machine(baselearner, Xtable, \u2202L\u2202F) fit!(basemachine) \u03c1 = -1.0 if (linesearch) # assuming least squares y\u0303 = y - F[m-1](X) x\u0303 = predict(basemachine, X) \u03c1 = dot(y\u0303,x\u0303)/dot(x\u0303,x\u0303) end F[m] = x->(F[m-1](x) + stepsize*\u03c1*predict(basemachine,x)) end return(F) end @load DecisionTreeRegressor pkg=BetaML baselearner = BetaML.Trees.DecisionTreeRegressor(max_depth=2) X = rand(200, 1) f(x) = sum(x.^2) \u03c3 = 0.1 y = f.(eachrow(X)) + \u03c3*randn(size(X,1)) loss(Y,Y\u0302) = sum( (y-y\u0302)^2 for (y, y\u0302) \u2208 zip(Y, Y\u0302) ) F = gradientboost(y,X, loss, baselearner,stepsize=0.1, linesearch=true, steps=100) using Plots Plots.pyplot() scatter(X, y) p = sortperm(vec(X)) xs = X[p] plot!(xs, f.(xs)) for m \u2208 eachindex(F) plot!(xs, F[m](reshape(xs, size(X))), alpha=m/length(F), colour=:black,legend=false) end plot!() import BetaML \u2714","title":"Example Code"},{"location":"gb/#alternate-baselearner","text":"","title":"Alternate baselearner"},{"location":"gb/#packages","text":"The above code is just for demonstration. It is better to use a package for gradient boosting. We will focus on gradient boosting trees. The two leading packages are XGBoost and LightGBM. These packages are written in C and have Julia (and many other languages) interfaces. LightGBM is newer, supported by Microsoft, and can be faster. EvoTrees is a pure Julia implementation and uses some of the same algorithm improvements that LightGBM does. Benchmarks on the EvoTrees github readme show that it can outperform XGBoost. #lgbm = @load LGBMRegressor #xgboost = @load XGBoostRegressor #evo = @load EvoTreeRegressor","title":"Packages"},{"location":"gb/#bibliography-bibliography","text":"Bakhitov, Edvard, and Amandeep Singh. 2022. \u201cCausal Gradient Boosting: Boosted Instrumental Variable Regression.\u201d In Proceedings of the 23rd ACM Conference on Economics and Computation , 604\u20135. EC \u201922. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3490486.3538251 . Bojer, Casper Solheim, and Jens Peder Meldgaard. 2021. \u201cKaggle Forecasting Competitions: An Overlooked Learning Opportunity.\u201d International Journal of Forecasting 37 (2): 587\u2013603. https://doi.org/ https://doi.org/10.1016/j.ijforecast.2020.07.007 . Dombry, Cl\u00e9ment, and Jean-Jil Duchamps. 2021. \u201cInfinitesimal Gradient Boosting.\u201d Friedman, Jerome H. 2001. \u201cGreedy Function Approximation: A Gradient Boosting Machine.\u201d The Annals of Statistics 29 (5): 1189\u20131232. http://www.jstor.org/stable/2699986 . \u2014\u2014\u2014. 2002. \u201cStochastic Gradient Boosting.\u201d Computational Statistics & Data Analysis 38 (4): 367\u201378. https://doi.org/ https://doi.org/10.1016/S0167-9473(01)00065-2 . Kueck, Jannis, Ye Luo, Martin Spindler, and Zigan Wang. 2017. \u201cEstimation and Inference of Treatment Effects with $L_2$-Boosting in High-Dimensional Settings.\u201d Luo, Ye, and Martin Spindler. 2016. \u201cHigh-Dimensional $L_2$Boosting: Rate of Convergence.\u201d Makridakis, Spyros, Evangelos Spiliotis, and Vassilios Assimakopoulos. 2022. \u201cM5 Accuracy Competition: Results, Findings, and Conclusions.\u201d International Journal of Forecasting . https://doi.org/ https://doi.org/10.1016/j.ijforecast.2021.11.013 . Yang, Jui-Chung, Hui-Ching Chuang, and Chung-Ming Kuan. 2020. \u201cDouble Machine Learning with Gradient Boosting and Its Application to the Big n Audit Quality Effect.\u201d Journal of Econometrics 216 (1): 268\u201383. https://doi.org/ https://doi.org/10.1016/j.jeconom.2020.01.018 . Yue, Mu, Jialiang Li, and Baoluo Sun. 2022. \u201cConditional Sparse Boosting for High-Dimensional Instrumental Variable Estimation.\u201d Journal of Statistical Computation and Simulation 92 (15): 3087\u20133108. https://doi.org/10.1080/00949655.2022.2056739 . Zhang, Tong, and Bin Yu. 2005. \u201c Boosting with early stopping: Convergence and consistency .\u201d The Annals of Statistics 33 (4): 1538\u201379. https://doi.org/10.1214/009053605000000255 . Zhou, Yichen, and Giles Hooker. 2018. \u201cBoulevard: Regularized Stochastic Gradient Boosted Trees and Their Limiting Distribution.\u201d","title":"Bibliography [bibliography]"},{"location":"license/","text":"The notes are licensed under a Creative Commons Attribution-ShareAlike 4.0 International License and were written by Paul Schrimpf.","title":"License"}]}