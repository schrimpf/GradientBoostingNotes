---
title       : "Gradient Boosting"
subtitle    : ""
author      : Paul Schrimpf
date        : `j using Dates; print(Dates.today())`
bibliography: "../ml.bib"
options:
      out_width : 100%
      wrap : true
      fig_width : 800
      dpi : 192
---

[![](https://i.creativecommons.org/l/by-sa/4.0/88x31.png)](http://creativecommons.org/licenses/by-sa/4.0/)

This work is licensed under a [Creative Commons Attribution-ShareAlike
4.0 International
License](http://creativecommons.org/licenses/by-sa/4.0/)



$$
\def\indep{\perp\!\!\!\perp}
\def\Er{\mathrm{E}}
\def\R{\mathbb{R}}
\def\En{{\mathbb{E}_n}}
\def\Pr{\mathrm{P}}
\newcommand{\norm}[1]{\left\Vert {#1} \right\Vert}
\newcommand{\abs}[1]{\left\vert {#1} \right\vert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$

```julia; echo=false; results="hidden"
markdown = try
  "md" in keys(WEAVE_ARGS) && WEAVE_ARGS["md"]
catch
  false
End

if !("DISPLAY" ∈ keys(ENV))
  # Make gr and pyplot backends for Plots work without a DISPLAY
  ENV["GKSwstype"]="nul"
  ENV["MPLBACKEND"]="Agg"
end
# Make gr backend work with λ and other unicode
ENV["GKS_ENCODING"] = "utf-8"

using GradientBoostingNotes
docdir = joinpath(dirname(Base.pathof(GradientBoostingNotes)), "..","docs")

using Pkg
Pkg.activate(docdir)
Pkg.instantiate()
```

# Introduction

Variants of gradient boosting decision trees often win prediction
contests (e.g. @bojer2021 and @m5accuracy ). @yang2020 examine the
peformance of various estimation methods in double machine
learning. They compare gradient boosted trees, random forests, lasso,
neural networks, and support vector machines, and find that gradient
boosting performs best.

# Decision Trees

See [this.](https://schrimpf.github.io/NeuralNetworkEconomics.jl/ml-methods/#random-forests)

# Gradient Boosting

Gradient boosting was first described in detail by @friedman2001, but the

and its stochastic
variant in @friedman2002. Gradient boosting deals with the following
regression or classification problem.

$$
\min_{F \in mathcal{F}} E_x \left[ E_y\left[ L(y, F(x)) | x \right] \right]
$$

where $L$ is some loss function and $\mathcal{F}$ is some family of
functions. For example if

$L$ is sqaured error and $\mathcal{F}$ is
polynomials of $x$, then this problem becomes series regression.

Gradient boosting focuses on the case where $\mathcal{F}$ is an additive sum of "simple" learners.

$$
 \mathcal{F} = \{ F(x;\beta, a) = \sum_{m=1}^M \beta_m h(x;a_m)
$$

For example, if
$$
h(x; a_m) = \psi(x'a_m)
$$
where $\psi$ is some
activation function, then $\mathcal{F}$ would be a single layer neural
network of width $M$. Gradient boosting is most often used when
$h(x;a_m)$ is a shallow decision tree.

Fully minimizing over $\beta$ and $a$ can be difficult. Therefore,
gradient boosting computes an approximate minimum through a greedy,
stagewise approach.


## Gradient Descent in Function Space

To motivate gradient boosting, consider what gradient descent in the
original problem looks like. Let
$Q(F) = E_x \left[ E_y\left[ L(y,F(x)) | x \right] \right]$
denote the objective function.
Then gradient descent would first choose an initial $F_0(m)$, and then:

1. Calculate
$$
\frac{\partial Q(F_{m-1}}{\partial F} = E_y\left[\frac{\partial L(y, F_{m-1}(x))}{\partial F(x)} | x \right]
$$

2. Set $F_m = F_{m-1} - \nu \frac{\partial Q(F_{m-1}}}{\partial F}$

3. Repeat

Optionally, the step-size, $\nu$ could be adjusted and/or a line search could be added.

## Gradient Boosting Algorithm

With the constraint that $F(x) = \sum \beta_m h(x; a_m)$, we cannot
always update $F$ in the direction of the gradient. Instead, we choose
a direction that obeys the constraint and is close to the gradient. Specifically,

1. Choose $a_m$ to approximate the gradient:
$$
a_m = \argmin_a \sum_{i=1}^N \left( \frac{\partial -L(y_i, F_{m-1}(x_i))}{\partial F(x)} - h(x_i; a) \right)^2
$$
2. Line search:
$$
\rho_m = \argmin_\rho \sum_{i=1}^N L\left( y_i, F_{m-1}(x_i) + \rho h(x_i;a_m) \right)
$$
3. Update:
$$
F_m(x) = F_{m-1}(x) + \nu \rho_m h(x; a_m)
$$
4. Repeat

Notice that when $L$ is squared error loss, then step 1 becomes
fitting $h(x_i;a)$ to the residuals. This idea of updating a model by
fitting the residuals existed before @friedman2001 , and is where the
name "boosting" originates.

## Regularization

There are at least three parameters that affect model complexity: the
number of iterations, $M$, the step size / shrinkage parameter, $\nu$,
and the complexity of $h(x,a)$.

In the case of trees, the depth of $h(x,a)$ has a clear implication
for type of functions that the model can well approximate. If $h(x,a)$
are stubs -- trees with a single split, then each $h(x,a)$ is a
function of only one component of $x$. Therefore we should expect to
be able to approximate additively separable functions like

$$
f_1(x_1) + f_2(x_2) + ... + f_k(x_k)$
$$

well. Trees with two splits can accomodate pairwise interactions, etc.

## Statistical Properties

@zhang2005 has consistency and convergence rate results. Whether those
results are compatible with double machine learning is not yet clear
to me.

@zhou2018 also appears relevant. It gives pointwise asymptotic
normality. Like Athey et al's results for forests, the class of
functions considered is too rich to get fast convergence rates.

@dombry2021 ODE view of boosting

In high dimensional settings (i.e. $x$ is high dimensional) there is a
relationship between boosting and lasso, see @luo2016 and @kueck2017.

Note: A creative definition of $x$ as indicators that your base tree
can generate might make @luo2016 results applicable to boosting
trees. The key would be verifying the sparse eigenvalue condition A.2,
and the restriction that the "true" function has a sparse
representation in terms of these indicators.


# Example Code

```julia
import Zygote
import LinearAlgebra: dot
using MLJ
import MLJBase

function gradientboost(y, X, loss, baselearner; steps=100, stepsize=0.1, linesearch=false)
  F = Vector{Function}(undef, steps)
  F[1] = x->mean(y)*ones(size(X,1))
  Xtable = MLJBase.table(X)
  if linesearch
    @warn "linesearch=true assumes squared error loss"
  end
  for m ∈ 2:steps
    ∂L∂F=Zygote.gradient(ŷ->loss(y, ŷ),F[m-1](X))[1]
    basemachine = machine(baselearner, Xtable, ∂L∂F)
    fit!(basemachine)
    ρ = -1.0
    if (linesearch) # assuming least squares
      ỹ = y - F[m-1](X)
      x̃ = predict(basemachine, X)
      ρ = dot(ỹ,x̃)/dot(x̃,x̃)
    end
    F[m] = x->(F[m-1](x) + stepsize*ρ*predict(basemachine,x))
  end
  return(F)
end

@load DecisionTreeRegressor pkg=BetaML
baselearner = BetaML.Trees.DecisionTreeRegressor(maxDepth=2)
X = rand(200, 1)
f(x) = sum(x.^2)
σ = 0.1
y = f.(eachrow(X)) + σ*randn(size(X,1))
loss(Y,Ŷ) = sum( (y-ŷ)^2 for (y, ŷ) ∈ zip(Y, Ŷ) )
F = gradientboost(y,X, loss, baselearner,stepsize=0.1, linesearch=true, steps=100)

using Plots
scatter(X, y)
p = sortperm(vec(X))
xs = X[p]
plot!(xs, f.(xs))
for m ∈ eachindex(F)
  plot!(xs, F[m](reshape(xs, size(X))), alpha=m/length(F), colour=:black,legend=false)
end

function showplot(m)
  scatter(X, y)
  p = sortperm(vec(X))
  xs = X[p]
  plot!(xs, f.(xs))
  plot!(xs, F[m](reshape(xs, size(X))), alpha=m/length(F), colour=:black,legend=false)
end


```

# Packages

The above code is just for demonstration. It is better to use a
package for gradient boosting. We will focus on gradient boosting
trees. The two leading packages are XGBoost and LightGBM. These
packages are written in C and have Julia (and many other languages)
interfaces. LightGBM is newer, supported by Microsoft, and can be
faster. EvoTrees is a pure Julia implementation and uses some of the
same algorithm improvements that LightGBM does. Benchmarks on the
[EvoTrees](https://github.com/Evovest/EvoTrees.jl) github readme show
that it can outperform XGBoost.

```julia
@load LGBMRegressor
@load XGBoostRegressor
@load EvoTreeRegressor


```


# Bibliography
